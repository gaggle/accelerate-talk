<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>reveal.js</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/white.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement('link')
      link.rel = 'stylesheet'
      link.type = 'text/css'
      link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css'
      document.getElementsByTagName('head')[0].appendChild(link)
    </script>

    <style>
        .reveal section img {
            border: none;
            box-shadow: none;
            margin: 0;
        }
    </style>
</head>
<body>
<div class="reveal">
    <div class="slides">
        <section data-notes="So, a new book came out recently, and it's really full of insights into software development. I want to speak to you about what it says, that we can do, to work smarter, and how it uses a very scientific approach to measure how effectively software teams create value. The research yields a number of quite specific suggestions that we can do to improve our work, and some of them are surprising and goes against how we do things.

The book is called &#34;Accelerate">
            <h2>The science of software development</h2>
            <h4>Practical tips for working smarter</h4>
        </section>

        <section data-notes="the science of lean and devops; building high performing organizations.&#34;
But, what is the science here? What does &#34;high performing&#34; mean?
To answer that we have to first understand a bit about what the data is, and how its analyzed. And once we do, then we can make sense of the suggestions that all this research points to.

Okay?">
            <img class="stretch" data-src="static/accelerate-cover.png" alt="Cover of Accelerate book"/>
        </section>

        <section
                data-notes="So, the research is very robust, it's based on surveys sent to a large number of people, around 20 to 30 *thousand* respondents, every year for 4 years, from all around the">
            <img class="fragment" data-src="static/demographics-survey-map-2018.png"
                 alt="Geographical map showing the entire world was surveyed"
                 data-notes="world. And those participants are from all kinds of industries, in tiny to huge companies, with all kinds of different ways of making software.
Participants were people and organizations familiar enough with the term &#34;DevOps&#34; to answer the surveys, so, in all this data everyone knows about things like infrastructure-as-code, and continuous integration.
The kind of work we do fits right into all this. The kind of product we make, our size, our technology, our process, it's all right in the middle of all this. This is definitely our crowd.

So, the data is relevant to us, and it's a big sample, but what did they find?"/>
        </section>

        <section>

            <section data-notes="First let's decode what's meant with &#34;high performing&#34;. Why should we believe that their idea of good is worth pursuing?

So, measuring performance of software teams has a troubled past.
For example early attempts tried measuring lines of code, or tracking how busy the people are, but that was too naive. Lines of code and busyness do not tell you how well the team delivers value.
Or there was using team velocity, so the more story-points burnt the better, but that doesn't work, in part because the teams end up pitted against each other, because helping another team can hurt your own velocity.
So, informed by past attempts, this research is careful to focus on measuring globally positive outcomes. That is, outcomes have to be good for the entire organisation.
The entire book really is about scientifically identifying which things we, in software teams, do, that leads to good performance for the company. By measuring how teams work, and how their companies perform, they can build statistical models of which variables control what outcomes.

And the data is quite remarkable: It identifies that the best teams are <b>twice<b> as likely to exceed company goals for, profitability, productivity, and customer satisfaction. Twice as likely.
So, we should want to know about this performance stuff, right? So, how do we actually measure team performance then?

Well, it all comes down to just four metrics. Again these are shown in the statistics to be linked to company performance. And they're actually surprisingly easy to go through.

So, first there's">
                <h2 class="fragment" data-notes="delivery lead time, that's how long it takes for code to get to production. Shorter is better, because, this is how fast we deliver value to our customers.
Concretely the survey asked how long it takes from code committed, to code successfully running in production, and participants had to choose one of these options:">
                    Delivery lead time</h2>
                <ul class="fragment" data-notes="Alright? That's all there is to delivery lead time.

The second performance metric, that is linked to company performance, is">
                    <li>Less than one hour</li>
                    <li>Less than one day</li>
                    <li>Between one day and one week</li>
                    <li>Between one week and one month</li>
                    <li>Between one month and six months</li>
                    <li>More than six months</li>
                </ul>
            </section>

            <section data-notes="deployment frequency. More frequent deploys reduces risk because each deploy has less features batched into them, and that removes those huge dumps of features that all have to go live at once.

Deployment here means deployment to production. Participants were asked how often they deploy code to their primary service, and had to chose one of these answers:">
                <h2>Deployment frequency</h2>
                <ul class="fragment"
                    data-notes="Okay, so far so good, lead time and deployment frequency captures performance tempo. But what if going fast comes at the expense of stability? We also need to measure">
                    <li>On demand (multiple deploys per day)</li>
                    <li>Between once per hour and once per day</li>
                    <li>Between once per day and onc per week</li>
                    <li>Between once per week and onc per month</li>
                    <li>Between once per month and once every six months</li>
                    <li>Fewer than once every six months</li>
                </ul>
            </section>

            <section
                    data-notes="quality. But.., what's quality? A classic reliability metric is to measure &#34;how much time between failures&#34;, but the thing with software is that it's so complex and always changing that failure is really inevitable.

So the more relevant question becomes &#34;How quickly is service restored?&#34;. And <b>that</b> is a widely used metric known as">
                <h2>Quality</h2>
            </section>

            <section>
                <h2 data-notes="mean time to restore.
So, participants were asked how long it generally takes to restore service when an incident occurs, like an unplanned outage or service impairment. And they had to pick one of these options">
                    Mean Time To Restore (MTTR)
                </h2>
                <ul class="fragment" data-notes="And the last to go through is">
                    <li>Less than one hour</li>
                    <li>Less than one day</li>
                    <li>Between one day and one week</li>
                    <li>Between one week and one month</li>
                    <li>Between one month and six months</li>
                    <li>More than six months</li>
                </ul>
            </section>

            <section data-notes="change fail percentage. This is tracking how many deploy or configuration changes causes a failure. The question asked was what percentage of all changes to a system result in degraded service?

So, y'know, any time we have to patch, or roll forward, or apply a hot fix, or redo work, that all counts towards this percentage.">
                <h2>Change fail percentage</h2>
            </section>

            <section data-notes="And that's it, those are the four performance metrics..
Now you might be thinking that these metrics don't seem to cover the entire software development process. Andâ€¦ you're right! No research perfectly captures everything, no research deals with every single variable.
The focus here is on the delivery of value via software, and how that relates to business performance. And based on all the survey data these four metrics statistically do improve that.

Then the researchers identified three distinct groups of responses, and I want to stress that the algorithm they used to find those groups has no understanding of &#34;good&#34; or &#34;bad&#34; responses. Okay?, they're totally unbiased groupings. And these three groups appeared consistently. And compared to how the companies of each group performed, the researchers labelled these groups High, Medium, and Low performers.">
                <img data-src="static/software-delivery-performance.png"
                     alt="List of the four performance metrics"/>
            </section>

            <section data-notes="And we can see here high performers have multiple deploys per day, changes go to production in less than an hour, they restore service outages in less than an hour, and they have a low change failure rate.

Perhaps surprisingly this shows that high performers do not trade off stability and quality. High performers do better in all measurements.

And yet, how often do we hear or say that we can move fast by trading off quality? The data shows that's just not the case. Next time any one of us hears the argument to cut a corner to go fast, think about this finding, remember that ">
                <img data-src="static/software-delivery-performance-for-2017.png"
                     alt="Table showing performance metrics for 2017"/>
            </section>

            <section data-notes="moving fast means high stability and quality.
Is that counter-intuitive to hear? I think for some it is, but even if you were already on-board with this I think the science and the hard data to back up that claim is new.

At this point I wondered, how do we rate?">
                <h4>Finding 0</h4>
                <h2>Moving fast means high stability and quality</h2>
            </section>

            <section data-notes="And, well, I don't have all the numbers.

I think we can answer Deployment frequency, for PFG at least we're almost realy to go to weekly releases now, through great work from Core and QEs and everyone who's worked on that.
And I think we can speak about MTTR, we had some recent incidents that lasted several days, right? Our current drive is to focus on stability, so I'm really looking forward to seeing how much we move the needle on this metric in the next months.
But lead time? And Change failure rate? I don't know them, but, I'd like to discuss us capturing them.

Because, what <b>if</b> we can all agree that we actually do want to track our performance like this? What if each team reported these metrics, and we discussed them together? I think that would provide a lot of insights, by quantifying the successes of certain ways each team works. One team might improve, say, deployment frequency above the others, and we could all learn from that. We'd inspire, and be inspired by, each other. I think we could really help each other to become more high performing, using the shared language of this performance model.">
                <img data-src="static/software-delivery-performance-for-2017.png"
                     alt="Table showing performance metrics for 2017"/>
            </section>

        </section>

        <section>

            <section data-notes="Okay, so, I think that was already pretty interesting. But I want to go deeper, because there are even more interesting and actionable findings to talk about.

I want to tell you about two more findings, these are practical tips we could start on today, but.. to get there we have to understand one more statistical construct. We've gone over how to measure performance, but to dig deeper we have to tackle the topic of">
                <h2 class="fragment"
                    data-notes="culture, But.. what's culture?, that's a word with many meanings.. What we need is more specifically known as">
                    Culture</h2>
            </section>

            <section>
                <h2 data-notes="organizational culture. Because agile and DevOps are as much about cultural changes as they are about technical changes.

There is a lot of detail to this topic, but I want to skip right to the end, where, after a lot of analysis, a model was chosen called the">
                    Organizational Culture</h2>
            </section>

            <section
                    data-notes="Westrum Culture model. That's an established and well-known model in the social sciences, it's been around for like 15 years and had its origins in some pretty serious industries like health-care and aviation.
It's a model that captures the impact culture has on organizational performance, and just like the software delivery model, it's actually very easy to gather the metrics that go into this one. I'll keep it brief and just say it boils down to half a dozen questions that participants rate their agreement to, such as, quote, &#34;On my team, responsibilities are shared&#34;.

But two things to know: First of all, the research by Dr. Westrum identified three types of organizational cultures,">
                <h2>Westrum Culture model</h2>
            </section>

            <section data-notes="those are Pathological, Bureaucratic, and Generative cultures. Generative culture is what we aim for, that's where we work together to accomplish goals, and on doing so well.

But secondly, the amazing thing with this Westrum culture model, is that Westrum showed it's not simply that culture <b>correlates</b> to organizational performance, it's actually predictive. Okay? He showed that a generative culture <b>causes</b> better organizational performance. I personally resisted that a bit when I first read it, but there are whitepapers and research to back it up. What I read boils down to a lot of case studies and statistics that show how changes to culture really does affect the companies, and that this model captures those changes.

So the book's research took all this, and combined it with their own data, and they also found that the Westrum culture predicts organizational performance. Okay? So it also works in software.">
                <img data-src="static/westrums-typology-of-organizational-culture.png"
                     alt="Graph showing details of each type of culture"/>
            </section>

            <section
                    data-notes="This graph comes from all that analysis, and it shows this predictive relationship. The &#34;software delivery performance&#34; box up on the right, is the one we went through before, and here we see that if culture is improved then the delivery performance and organizational performance is also improved.
Are these bold statements to anyone? I don't know about you, but I didn't know about this kind of research at all. Of course there are no guarantees in statistics, but these kinds of predictive relationships are about as powerful as it gets in social science. With this we see how we can work towards likely success.

I sometimes want to ask, like, &#34;why does it work this way?&#34;, or &#34;what is the exact reason culture matters to performance?&#34;. Because, at this point everything has been kind of abstract, right? Performance is nice, but, how does it relate to my daily work?
We will actually go into some concrete examples here in a second, but it's valuable to understand these relationships I think, even though they are abstract, because it focuses our discussions in the right direction. We don't have to discuss every little detail on what directions to go in, if we can agree that these models are good enough for us.

And, actually we do already use this kind of thinking. Our yearly survey has the kind of questions that informs the Westrum model, so I think whoever does our surveys are using this science too. Although, I think a year is a long time between surveying. Some companies ask the Westrum questions quarterly, to better steer their culture. Personally, I'd support that, I think it would allow us to more quickly see if our changes are having an effect on our culture.

But of course the research didn't stop here. All that survey data shows another very interesting predictive relationship: That technical practices like Continuous Delivery">
                <img data-src="static/westrum-organizational-cultures-outcomes.png"
                     alt="Graph showing Westrum culture impacts software delivery and organizational performance"/>
            </section>

            <section data-notes="improves culture, as measured by that Westrum model. That's what we see here. We can improve our culture, by implementing continuous delivery. Does that make sense? I know we haven't touched on that &#34;Continuous Delivery&#34; box yet, but it's another statistical construct, just like the other ones we've been through. This one is based on things like test automation and monitoring and more.
By the way the blank box has some details unrelated to this talk, but we can look at those later if you want.

The point here is that we can couple these constructs together, to form a statistically valid argument that shows that">
                <img data-src="static/westrum-organizational-cultures-drivers-highlight.png"
                     alt="Graph showing Continuous Delivery impacts Westrum culture"/>
            </section>

            <section data-notes="continuous delivery causes better software and organizational performance!
Wow! Except.. heh, okay, we.. actually already knew that, didn't we? :) <b>Of course</b> continuous delivery improves software delivery performance.. it's what our whole industry has turned towards for a decade. We should be surprised if there was no impact, if somehow things like test automation and monitoring didn't matter to delivery performance.
But that's often the case with science, it shows what we already know, but, by looking at it through these models, we validate our knowledge with real actual numbers, and crucially the models explain what we can do to impact them.

And now we're reaching the level where things impact our daily work. Because in that Continuous Delivery model, the things that impact it, are very concrete, <b>they're</b> the findings I want to tell you about.">
                <img data-src="static/westrum-organizational-cultures-drivers-and-outcomes-highlight.png"
                     alt="Graph showing Continuous Delivery and Lean Management impacts performance"/>
            </section>

        </section>

        <section>

            <section
                    data-notes="But.. That was a lot. That was a lot to take in, right? But, all that context was important, because now we see how these findings aren't bogus. When we say &#34;going fast means high stability and quality&#34;, that's really a powerful statement built on data and validated models. It's a scientific statement.
Of course, it's, not like every word of this is, some holy truth.. At the end of the day this is social science and statistics so, there is a limit to the precision. And the models and the data can and should be criticised. But when we criticise we must take into account the thousands of data-points that underpin these findings; their careful use of science. These findings really do maximize the likelihood of success.

So, ultimately the book identifies 24 specific technical and managerial practices that impacts organizational performance. The complete set of capabilities is beyond this talk, but today I've picked two that relates directly to how we work. They're both technical capabilities, because, we're mostly technical people here, and I want to leave you with advice that hits close to home. But the remaining ones also focus on process, and management, and security, architecture, and more. They're all well worth looking into.

But, let's go, the first capability I want to talk about is">
                <div class="fragment" data-notes="trunk based development. Aka master-based development. It's the way of working that is the opposite of long-lived feature branches.

The data is really clear on this: High performing teams have fewer than 3 active branches at any time, their branches have very short lifespans of less than a day, and the teams never have code freeze periods. These results are completely independent of team size, org size, or industry.

Trunk based development is strongly correlated with Continuous Delivery, and so we know that, that means the company will do better.

Is that surprising? Certainly the science here was new to me. One theory for why long lived branches are bad, is that they discourage small refactorings, because there are always big changes on the horizon.

It is very rare that I can get behind absolute statements, but the researchers stress that they are really clear on this finding: Do not have long lived branches.

Okay? Food for thought for some of us, I think.

Let's try one more. Let's talk about">
                    <h4>Finding 1</h4>
                    <h2>Trunk based development</h2>
                </div>
            </section>

            <section data-notes="version control. Of all production artifacts. Version control has a very strong correlation to continuous delivery.

Now, maybe saying version control sounds innocent, but recall that all of our database is really not versioned. And what about any servers or services that you wouldn't want to lose, because they have special changes applied directly to them, that aren't captured in code?

The research identifies a whole set of things that are important to store in version control: All production artifacts, all application code, all application and system configurations, and all scripts for building and configuring the environment. All of those strongly correlates to continuous delivery, so this is another finding that has a great impact on team- and org-performance.

Interestingly the data shows that it is not the application code that's most critical to version, it is actually the <b>configuration</b> code that has the most impact.

Is that surprising? I was definitely surprised that application code isn't the most critical thing to version. It's configuration code for the service, and the configuration of the environment it runs it. So no manual messing-about with AWS resources, or editing JSON files or values directly on a service.

Imagine we took this finding to heart, imagine our database, with all its configuration, its environment, its details, everything captured in version control. We could run that code to make us a database in a different environment, couldn't we? We could test changes in a development environment, instead of directly in production. Having a database where I don't have to fear every change catastrophically losing production data, that would be nice, wouldn't it? And the selects system should be versioned too. No more editing a select and it goes live the second its saved.

I think we intuitively know a lot of this already, but this research really validates that this has a huge impact on organizational performance. With this data we can see that we must get our systems into version control, it's not just some technical niceity. The data shows we're leaving money on the table by NOT doing this, because this is one the road for becoming high performers.
<pause>
So">
                <div>
                    <h4>Finding 2</h4>
                    <h2>Version control</h2>
                </div>
            </section>

            <section data-notes="those are all we have time for today, but I think it's been a lot to take in. If you're interested to hear about the other findings, then, let's meet up. Maybe there's even enough for another talk.. just, tell me, if you would be interested in that.

I think it's worth saying again that these findings are only from four years of research, so, of course it's worth discussing how each model and finding fits into our context. But a lot of this is also relatively simple to try, they're practical things we can do and then see how they work for us. I don't advocate that we must blindly adopt whatever it says in this book, but.. the scientific mindset <b>is</b> one I think we should pay a lot of attention to.

Because it's by measuring how we work, that we can introduce changes and see their impact. And if we do that, and we use it to continuously improve ourselves, to continuously tweak how we work to become ever more effective, then we <b>will</b> go all the way to the top.
The capabilities identified by this research involves everyone, I know today I only talked about technical findings, but the full set involves leadership, management, sales, support, everyone. We should all involve ourselves in this.

It is totally in our power to make these changes, we just have to grab the opportunity.

Thank you :), now, I'd love to hear which questions you have for me.">
                <h4>Findings</h4>
                <ul>
                    <li>Moving fast means <strong>high stability and quality</strong></li>
                    <li>Trunk based development</li>
                    <li>Version control <strong>everything!</strong></li>
                </ul>
                <img data-src="static/all-relationships-highlight.png"
                     alt="Graph showing full relationship of technical capabilities impacting software delivery and organizational performance"/>
            </section>

        </section>

        <section>
            <img data-src="static/extras/all-relationships.png" alt="Graph showing all relationships"/>
        </section>

        <section>
            <section>
                <h4>Finding 3</h4>
                <h2>Acceptance tests</h2>
                <h4 class="fragment">Primarily developed<br/>and maintained <u>by developers</u></h4>
            </section>
        </section>
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
  // More info about config & dependencies:
  // - https://github.com/hakimel/reveal.js#configuration
  // - https://github.com/hakimel/reveal.js#dependencies
  Reveal.initialize({
    dependencies: [
      {src: 'plugin/markdown/marked.js'},
      {src: 'plugin/markdown/markdown.js'},
      {src: 'plugin/notes/notes.js', async: true},
      {src: 'plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad() }},
      {src: 'plugin/zoom-js/zoom.js', async: true},
    ]
  })
</script>

<script>
  Reveal.addEventListener('fragmentshown', function (event) {
    if (event.fragment.classList.contains('remove-item')) {
      document.getElementById(event.fragment.dataset.remove)
        .classList.add('removed-item')
    }
  })
  Reveal.addEventListener('fragmenthidden', function (event) {
    if (event.fragment.classList.contains('remove-item')) {
      document.getElementById(event.fragment.dataset.remove)
        .classList.remove('removed-item')
    }
  })
</script>
</body>
</html>
